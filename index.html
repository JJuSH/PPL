<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION META TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Policy-labeled Preference Learning : Is Preference Enough for RLHF?</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Policy-labeled Preference Learning : Is Preference Enough for RLHF?</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><a href="#" target="_blank">Taehyun Cho</a><sup>*</sup>,</span>
              <span class="author-block"><a href="#" target="_blank">Seokhun Ju</a><sup>*</sup>,</span>
              <span class="author-block"><a href="https://hansungy.github.io/" target="_blank">Seungyub Han</a>,</span>
              <span class="author-block"><a href="#" target="_blank">Dohyeong Kim</a>,</span>
              <span class="author-block"><a href="#" target="_blank">Kyungjae Lee</a>,</span>
              <span class="author-block"><a href="#" target="_blank">Jungwoo Lee</a><sup>†</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Seoul National University<br>ICML 2025 Spotlight(top 2.6%)</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution, <sup>†</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Supplementary</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/YOUR REPO HERE" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://www.arxiv.org/abs/2505.06273" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="ai ai-arxiv"></i></span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Introduction</h2>
      <!-- 여기에 이미지 삽입 -->
      <figure class="is-fullwidth">
        <img src="static/images/PPL_0126_page-0001.jpg" alt="PPL Framework Overview">
        <figcaption class="has-text-centered is-size-7 mt-2">
          Figure 1. Overview of the PPL framework
        </figcaption>
      </figure>
      <p>
        Policy-labeled Preference Learning (PPL) is a novel RLHF framework designed to overcome the “likelihood mismatch” and stochasticity challenges that arise when learning from pairwise preference data collected under diverse, potentially suboptimal behavior policies. Unlike standard preference-based RL—which assumes all trajectories come from an optimal policy and infers rewards purely from rankings—PPL tags each trajectory segment with its generating policy and computes its regret (the gap between that policy’s performance and the optimal policy). This disentangles the effects of environmental randomness from policy suboptimality, yielding a more reliable learning signal.
      </p>
      <p>
        Building on this regret-based decomposition, PPL incorporates a contrastive KL regularizer that pulls the policy distribution toward preferred trajectories while pushing it away from less preferred ones. Theoretically, PPL defines a reward-equivalence class and shows that regret uniquely determines an optimal policy within that class, addressing reward sparsity and improving stability. Empirically, PPL consistently outperforms existing methods—including DPO, CPL, and P-IQL—on both homogeneous and heterogeneous offline datasets in MetaWorld manipulation tasks, and demonstrates robust success even under sparse-preference settings.
      </p>
    </div>
  </section>

  <!-- Method Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Method</h2>
      <p>
        Our key insight is that when you mix data from both optimal and suboptimal controllers, you get “likelihood mismatch”—differences caused by a poor policy look like random noise. To fix this, PPL tags each trajectory segment with the exact policy that generated it, then scores segments by their <em>regret</em> (i.e., how much worse the behavior policy did versus the optimal one).  
      </p>
      <!-- 여기에 이미지 삽입 -->
      <figure class="is-fullwidth">
        <img src="static/images/Likelihood_Mismatch_page-0001.jpg" alt="Likelihood Mismatch" style="width: 60%; height: auto; margin-top: 1rem;">
        <figcaption class="has-text-centered is-size-7 mt-2">
          Figure 2. Likelihood Mismatch
        </figcaption>
      </figure>
      <p>
        We train a preference model on these policy-labeled, regret-scaled segments using a convex loss that admits an efficient second-order update. In practice, this lets us disentangle true randomness from suboptimal behavior, yielding much cleaner reward estimates and more stable policy improvements.
      </p>
    </div>
  </section>

  <!-- Experiments Section -->
  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Experiments</h2>
      <!-- 여기에 이미지 삽입 -->
      <figure class="image is-16by9">
        <img src="static/images/env.png" alt="Metaworld Environment">
        <figcaption class="has-text-centered is-size-7 mt-2">
          Figure 3. Metaworld Environments
        </figcaption>
      </figure>
      <p><strong>Experimental Questions</strong><br>
      We evaluate PPL on three fronts:  
      1. <em>Offline learning with heterogeneous data</em>: Can PPL learn effectively from mixed-policy offline datasets?  
      2. <em>Impact of policy labels</em>: Does tagging each segment with its generating policy improve performance?  
      3. <em>Online RLHF applicability</em>: Can PPL’s regret-based framework be integrated into an online preference-based RL algorithm?</p>

      <p><strong>Setup & Baselines</strong><br>
      • <strong>Environments:</strong> Six MetaWorld robotic tasks using the same raw rollouts from Hejna et al. (2023).<br>
      • <strong>Data regimes:</strong><br>
      &nbsp;&nbsp;– <em>Homogeneous:</em> 100%-success oracle rollouts.<br>
      &nbsp;&nbsp;– <em>Heterogeneous:</em> Mix of 20% and 50% success-rate policies.<br>
      &nbsp;&nbsp;– <em>Preference density:</em> “Dense” (all-pair comparisons) vs. “Sparse” (one comparison per segment).<br>
      • <strong>Labeling:</strong> A pretrained SAC oracle samples 64-step segments and assigns regret-based labels (no critic retraining) to keep label noise uniform.<br>
      • <strong>Baselines:</strong>  
      – CPL (no policy labels)  
      – SFT (supervised fine-tuning via behavior cloning)  
      – P-IQL (reward-based RLHF + Implicit Q-Learning)</p>

      <p>Full details and reproducibility results appear in Appendices E–H.</p>
    </div>
  </section>

</body>
</html>
